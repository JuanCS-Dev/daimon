# üìö PROCESSO COMPLETO DE TREINAMENTO NOESIS
**Data:** 2025-12-09 20:39  
**Status:** DOCUMENTA√á√ÉO COMPLETA DO PROCESSO

---

## üéØ VIS√ÉO GERAL DO PIPELINE

```
[1] CRIAR EXEMPLOS MANUAIS (600)
    ‚Üì
[2] GERAR VARIA√á√ïES (5500)
    ‚Üì
[3] VALIDAR DATASET (6100 total)
    ‚Üì
[4] EXPORTAR PARA MODAL
    ‚Üì
[5] TREINAR COM UNSLOTH + QLORA
    ‚Üì
[6] AVALIAR COM TRIBUNAL
    ‚Üì
[7] MERGE LORA ‚Üí MODELO FINAL
    ‚Üì
[8] DEPLOY
```

---

## üìù ETAPA 1: CRIAR EXEMPLOS MANUAIS

### Estrutura Obrigat√≥ria (JSONL):

```json
{
  "id": "categoria_000",
  "category": "nome_categoria",
  "prompt": "Pergunta ou situa√ß√£o (m√≠nimo 10 chars)",
  "response_initial": "Resposta superficial/errada",
  "critique": "[VERITAS] Cr√≠tica verdade\n[SOPHIA] Cr√≠tica sabedoria\n[DIKE] Cr√≠tica justi√ßa",
  "response_revised": "Resposta profunda e correta (m√≠nimo 50 chars)",
  "reasoning": "Por que essa resposta √© melhor",
  "values_applied": ["verdade", "sabedoria"],
  "difficulty": "easy|medium|hard"
}
```

### Campos Obrigat√≥rios:
- ‚úÖ `id`: Identificador √∫nico
- ‚úÖ `category`: Categoria do exemplo
- ‚úÖ `prompt`: Pergunta/situa√ß√£o
- ‚úÖ `response_initial`: Resposta ruim (para Constitutional AI)
- ‚úÖ `critique`: Cr√≠tica do Tribunal (Veritas, Sophia, Dikƒì)
- ‚úÖ `response_revised`: Resposta boa ap√≥s cr√≠tica
- ‚úÖ `reasoning`: Justificativa da abordagem
- ‚úÖ `values_applied`: Lista de valores (m√≠nimo 1)
- ‚úÖ `difficulty`: "easy", "medium" ou "hard"

### Valores Dispon√≠veis:
- `verdade` (Veritas) - 40% peso
- `sabedoria` (Sophia) - 30% peso
- `justica` (Dikƒì) - 30% peso
- `florescimento`
- `alianca`
- `humildade`

### Arquivo:
```
data/training/seed_examples_philosophical.jsonl
```

---

## üîÑ ETAPA 2: GERAR VARIA√á√ïES

### Script: `scripts/generate_fast.py`

```bash
python3 scripts/generate_fast.py \
  --input data/training/seed_examples_philosophical.jsonl \
  --output data/training/generated/all_variations.jsonl \
  --count 5500
```

### T√©cnicas de Varia√ß√£o:
1. **Profundidade**: Adicionar "explique em profundidade"
2. **Simplifica√ß√£o**: "Explique de forma simples"
3. **Aplica√ß√£o**: "Como aplicar na pr√°tica"
4. **Hist√≥ria**: "Evolu√ß√£o hist√≥rica de..."
5. **Compara√ß√£o**: "Compare e contraste"
6. **Cr√≠tica**: "Limita√ß√µes e cr√≠ticas"

---

## ‚úÖ ETAPA 3: VALIDAR DATASET

### Script de Valida√ß√£o:

```python
import json

required = ["id", "category", "prompt", "response_initial", 
            "critique", "response_revised", "reasoning", 
            "values_applied", "difficulty"]

for line in open("dataset.jsonl"):
    ex = json.loads(line)
    
    # Check campos
    assert all(f in ex for f in required)
    
    # Check tipos
    assert isinstance(ex["values_applied"], list)
    assert ex["difficulty"] in ["easy", "medium", "hard"]
    
    # Check tamanhos
    assert len(ex["prompt"]) >= 10
    assert len(ex["response_revised"]) >= 50
    
    # Check alucina√ß√µes
    assert "TODO" not in ex["response_revised"]
    assert "FIXME" not in ex["response_revised"]
    assert "lorem ipsum" not in ex["response_revised"].lower()
```

### Estat√≠sticas Esperadas:
- ‚úÖ Total: 6100 exemplos
- ‚úÖ Taxa de sucesso: 100%
- ‚úÖ Sem alucina√ß√µes
- ‚úÖ Categorias: 20-30
- ‚úÖ Dificuldades: ~60% hard, ~30% medium, ~10% easy

---

## üì§ ETAPA 4: EXPORTAR PARA MODAL

### 4.1: Combinar Datasets

```bash
cd data/training

# Combinar base + varia√ß√µes
cat seed_examples_philosophical.jsonl \
    generated/all_generated_*.jsonl \
    generated/variations_*.jsonl \
    > exports/dataset_complete.jsonl
```

### 4.2: Criar Train/Eval Split (90/10)

```python
import json, random

random.seed(42)

examples = [json.loads(l) for l in open("exports/dataset_complete.jsonl")]
random.shuffle(examples)

split = int(len(examples) * 0.9)
train = examples[:split]
eval_set = examples[split:]

# Salvar
with open("exports/train.jsonl", 'w') as f:
    for ex in train:
        f.write(json.dumps(ex, ensure_ascii=False) + '\n')

with open("exports/eval.jsonl", 'w') as f:
    for ex in eval_set:
        f.write(json.dumps(ex, ensure_ascii=False) + '\n')
```

### 4.3: Upload para Modal Volume

```bash
# Verificar volume existe
modal volume list | grep noesis-training-data

# Se n√£o existe, criar
modal volume create noesis-training-data

# Upload
modal volume put noesis-training-data \
  data/training/exports \
  /dataset

# Verificar upload
modal volume ls noesis-training-data/dataset
```

---

## üöÄ ETAPA 5: TREINAR NO MODAL

### Configura√ß√£o do Ambiente:

**GPU**: L40S (48GB VRAM)  
**Modelo Base**: `meta-llama/Llama-3.1-8B-Instruct`  
**T√©cnica**: QLoRA (4-bit quantization)  
**Framework**: Unsloth (2x faster)

### Hiperpar√¢metros (training_config.yaml):

```yaml
training:
  num_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4  # Batch efetivo = 8
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.01
  
lora:
  r: 64              # Rank
  lora_alpha: 128    # Scaling (2x r)
  lora_dropout: 0    # Unsloth recomenda 0
  target_modules:
    - q_proj, k_proj, v_proj, o_proj
    - gate_proj, up_proj, down_proj
```

### Comandos de Treinamento:

```bash
# 1. Treinar 1 √©poca
modal run scripts/modal_train.py::train_epoch --epoch 0

# 2. Treinar pipeline completo (3 √©pocas)
modal run scripts/modal_train.py --epochs 3

# 3. Apenas avaliar checkpoint existente
modal run scripts/modal_train.py --test-only

# 4. Apenas fazer merge
modal run scripts/modal_train.py --merge-only
```

### Formato de Training (Llama 3.1 Chat):

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Voc√™ √© Noesis, um fil√≥sofo l√≥gico guiado por cinco valores...
<|eot_id|><|start_header_id|>user<|end_header_id|>

{{prompt}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

[AN√ÅLISE INTERNA]
{{critique}}
[FIM DA AN√ÅLISE]

{{response_revised}}<|eot_id|>
```

### Custos Estimados:

| Item | Custo | Tempo |
|------|-------|-------|
| 1 √©poca | ~$30 | ~2h |
| 3 √©pocas | ~$90 | ~6h |
| Avalia√ß√£o | ~$5 | ~30min |
| Merge | ~$10 | ~20min |
| **TOTAL** | **~$105** | **~8.5h** |

---

## üìä ETAPA 6: AVALIAR COM TRIBUNAL

### Fun√ß√£o: `evaluate_with_tribunal()`

```python
# Automatic ap√≥s cada √©poca
eval_result = evaluate_with_tribunal.remote(
    checkpoint_path="/data/checkpoints/epoch_0"
)

print(f"Avg Score: {eval_result['avg_score']:.2f}")
print(f"Pass Rate: {eval_result['pass_rate']:.1%}")
```

### M√©tricas do Tribunal:

- **Veritas** (Truth): 0.0-1.0
- **Sophia** (Wisdom): 0.0-1.0
- **Dikƒì** (Justice): 0.0-1.0
- **Total**: (Veritas√ó0.4 + Sophia√ó0.3 + Dikƒì√ó0.3)

### Thresholds:

- `>0.7`: ‚úÖ APPROVED
- `0.5-0.7`: ‚ö†Ô∏è CONDITIONAL
- `<0.5`: ‚ùå REJECTED

---

## üîÄ ETAPA 7: MERGE LORA

### Por Que Merge?

LoRA guarda apenas **adaptadores** (~200MB). Para deploy, precisa:
1. Carregar modelo base (8GB)
2. Aplicar adaptadores
3. **OU** fazer merge = modelo completo standalone

### Comandos:

```bash
# Via Modal
modal run scripts/modal_train.py --merge-only

# Manual (se tiver checkpoint local)
python3 << 'MERGE'
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="checkpoints/epoch_2",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True
)

model.save_pretrained_merged(
    "final_model",
    tokenizer,
    save_method="merged_16bit"  # Full 16-bit model
)
MERGE
```

### Output:

```
/data/merged/noesis-philosopher-v1/
‚îú‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ generation_config.json
‚îú‚îÄ‚îÄ model-00001-of-00004.safetensors
‚îú‚îÄ‚îÄ model-00002-of-00004.safetensors
‚îú‚îÄ‚îÄ model-00003-of-00004.safetensors
‚îú‚îÄ‚îÄ model-00004-of-00004.safetensors
‚îú‚îÄ‚îÄ model.safetensors.index.json
‚îú‚îÄ‚îÄ special_tokens_map.json
‚îú‚îÄ‚îÄ tokenizer.json
‚îî‚îÄ‚îÄ tokenizer_config.json
```

---

## üö¢ ETAPA 8: DEPLOY

### 8.1: Download do Modal

```bash
modal volume get noesis-training-data \
  /merged/noesis-philosopher-v1 \
  ./models/noesis-philosopher-v1
```

### 8.2: Test Local

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "./models/noesis-philosopher-v1",
    device_map="auto",
    torch_dtype="auto"
)

tokenizer = AutoTokenizer.from_pretrained(
    "./models/noesis-philosopher-v1"
)

prompt = "O que √© consci√™ncia artificial?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=512)
print(tokenizer.decode(outputs[0]))
```

### 8.3: Deploy Options

**Op√ß√£o A: vLLM (Recomendado para produ√ß√£o)**

```python
from vllm import LLM, SamplingParams

llm = LLM(model="./models/noesis-philosopher-v1")
output = llm.generate(prompt, SamplingParams(max_tokens=512))
```

**Op√ß√£o B: llama.cpp (CPU/Mobile)**

```bash
# Converter para GGUF
python convert.py models/noesis-philosopher-v1 \
  --outtype f16 \
  --outfile noesis.gguf

# Quantizar (opcional)
./quantize noesis.gguf noesis-q4_k_m.gguf Q4_K_M

# Run
./llama-cli -m noesis-q4_k_m.gguf -p "O que √© justi√ßa?"
```

**Op√ß√£o C: HuggingFace Hub**

```bash
huggingface-cli login
huggingface-cli upload \
  your-username/noesis-philosopher-v1 \
  ./models/noesis-philosopher-v1
```

---

## üõ†Ô∏è TROUBLESHOOTING

### Erro: "Dataset not found"

```bash
# Verificar volume
modal volume ls noesis-training-data/dataset

# Re-upload se necess√°rio
modal volume put noesis-training-data \
  data/training/exports /dataset
```

### Erro: "HuggingFace token invalid"

```bash
# Recriar secret
modal secret create huggingface-token HF_TOKEN=hf_...
```

### Erro: "Out of memory"

Reduzir batch size no `modal_train.py`:
```python
batch_size=1  # Era 2
gradient_accumulation=8  # Era 4
```

### Erro: "torch_dtype KeyError"

Vers√µes j√° corrigidas no script:
- `transformers==4.55.4`
- `trl==0.22.2`
- `unsloth==2025.9.7`

### Checkpoint corrompido

```bash
# Listar checkpoints
modal volume ls noesis-training-data/checkpoints

# Remover corrompido
modal volume rm noesis-training-data/checkpoints/epoch_X

# Retreinar do √∫ltimo bom
modal run scripts/modal_train.py::train_epoch --epoch X
```

---

## üìã CHECKLIST PR√â-TREINAMENTO

- [ ] 6100 exemplos validados (100% success rate)
- [ ] Train/eval split criado (90/10)
- [ ] Arquivos em `data/training/exports/`
- [ ] Volume Modal existe: `noesis-training-data`
- [ ] Secret HuggingFace configurado
- [ ] Dataset uploaded para `/dataset` no volume
- [ ] `modal_train.py` testado com `--test-only`
- [ ] Budget confirmado (~$105 USD)

---

## üìã CHECKLIST P√ìS-TREINAMENTO

- [ ] 3 √©pocas completadas
- [ ] Loss decrescente
- [ ] Tribunal score > 0.7
- [ ] Checkpoints salvos em `/checkpoints`
- [ ] Merge realizado
- [ ] Modelo final em `/merged`
- [ ] Download local do modelo
- [ ] Teste de gera√ß√£o funcionando
- [ ] Deploy strategy definido

---

## üìö REFER√äNCIAS

1. **Unsloth Docs**: https://docs.unsloth.ai
2. **Modal Docs**: https://modal.com/docs
3. **TRL (Transformers RL)**: https://huggingface.co/docs/trl
4. **QLoRA Paper**: https://arxiv.org/abs/2305.14314
5. **Constitutional AI**: https://arxiv.org/abs/2212.08073

---

## ‚úÖ PROCESSO COMPLETAMENTE DOCUMENTADO

Este documento captura TUDO o que foi aprendido sobre o processo de treinamento.
Nada foi assumido - tudo foi lido, explorado e validado.

**Data de Cria√ß√£o:** 2025-12-09 20:39:34  
**Status:** ‚úÖ COMPLETO E PRONTO PARA EXECU√á√ÉO

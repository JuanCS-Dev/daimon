# ðŸš€ PLANO DE SCALING 50X - NOESIS BIG DATA

**Data:** 2025-12-10
**Custo Atual:** $3 (6.1K exemplos)
**Meta:** $150-200 (300K+ exemplos)
**Escala:** 50x

---

## ðŸ“Š ESTRATÃ‰GIAS DE SCALING

### 1. DATASET SCALING (300K exemplos)

#### A. Self-Instruct + Evol-Instruct (150K exemplos)
```bash
# Usar Claude/GPT-4 para gerar variaÃ§Ãµes
# Custo: ~$50 (API calls)

python scripts/generate_synthetic_data.py \
  --base-examples 600 \
  --target 150000 \
  --method evol-instruct \
  --model claude-3-opus
```

**TÃ©cnicas:**
- **Breadth Evolution:** Adicionar constraints, contextos
- **Depth Evolution:** Aumentar complexidade, raciocÃ­nio
- **In-Breadth Evol:** Criar cenÃ¡rios paralelos
- **Mutation:** Transformar formato (codeâ†’essay, dialogueâ†’monologue)

#### B. Seed Data Diversification (100K exemplos)
```python
# Explorar novos domÃ­nios filosÃ³ficos
domains = [
    "Epistemologia Bayesiana",
    "Teoria dos Jogos EvolucionÃ¡ria", 
    "Fenomenologia Computacional",
    "Ã‰tica de IA AvanÃ§ada",
    "Meta-filosofia da CiÃªncia",
    "LÃ³gica Paraconsistente",
    "Filosofia da Mente Conectada",
    "NeuroÃ©tica Computacional"
]
```

#### C. Cross-Domain Knowledge (50K exemplos)
- Filosofia + MatemÃ¡tica AvanÃ§ada
- FÃ­sica QuÃ¢ntica + ConsciÃªncia
- Biologia + Ã‰tica
- Economia + Teoria da DecisÃ£o
- HistÃ³ria + Epistemologia

---

### 2. COMPUTE SCALING (GPU Upgrade)

#### OpÃ§Ã£o A: Multi-GPU Training
```yaml
# modal_train.py - Atualizar para DDP
gpu: "A100-80GB:4"  # 4x A100 (320GB VRAM)
batch_size: 8       # Por GPU = 32 efetivo
gradient_acc: 2     # Batch efetivo = 64
```

**Custo:** ~$12/hora Ã— 4h = **$48**
**Throughput:** 8x faster

#### OpÃ§Ã£o B: H100 Single GPU
```yaml
gpu: "H100"  # 80GB, 3x faster que A100
batch_size: 4
gradient_acc: 8  # Batch = 32
```

**Custo:** ~$8/hora Ã— 5h = **$40**
**Throughput:** 3x faster

---

### 3. MODEL SCALING (Larger Base Models)

#### OpÃ§Ã£o A: Llama-3.1-70B
```python
base_model = "meta-llama/Llama-3.1-70B-Instruct"
# Precisa: 4x A100 ou 2x H100
# QLoRA adapters: ~5GB
```

**Custo:** $80-120 para 3 Ã©pocas
**Ganho:** Capacidade 8x maior

#### OpÃ§Ã£o B: Mixtral 8x22B (MoE)
```python
base_model = "mistralai/Mixtral-8x22B-Instruct-v0.1"
# Sparse MoE: sÃ³ ativa 2 experts por token
# Mais eficiente que dense
```

**Custo:** $60-100
**Ganho:** 22B params, eficiÃªncia de 3B

---

### 4. TRAINING OPTIMIZATION

#### A. Learning Rate Scheduling
```python
# Atual: Cosine simples
# Upgrade: Cosine with Warm Restarts

scheduler = CosineAnnealingWarmRestarts(
    optimizer,
    T_0=1000,      # Restart a cada 1000 steps
    T_mult=2,      # Dobrar perÃ­odo
    eta_min=1e-6   # LR mÃ­nimo
)
```

#### B. Curriculum Learning
```python
# Fase 1: Easy examples (epochs 0-1)
# Fase 2: Medium examples (epochs 2-4)
# Fase 3: Hard examples (epochs 5-7)
# Fase 4: Mix all (epochs 8-10)

# Resultado: ConvergÃªncia mais rÃ¡pida e estÃ¡vel
```

#### C. Mixed Precision + Flash Attention 2
```python
# JÃ¡ usa bfloat16
# Upgrade: Flash Attention 2 (2x faster)

pip install flash-attn --no-build-isolation
model = FastLanguageModel.from_pretrained(
    ...,
    use_flash_attention_2=True  # ATIVAR!
)
```

---

## ðŸ’° BUDGET BREAKDOWN (50x Scale)

| Item | Atual | 50x Scale | Custo |
|------|-------|-----------|-------|
| Dataset generation | Manual | API synthetic | $50 |
| Training compute | L40S 1h | H100 8h | $64 |
| Validation runs | 0 | 3 iterations | $20 |
| Merge + Export | $0 | Same | $2 |
| **TOTAL** | **$3** | **50x data+model** | **$136** |

---

## ðŸŽ¯ EXECUTION PLAN

### Phase 1: Data Generation (1 week)
```bash
# Dia 1-2: Evol-Instruct 100K
python scripts/generate_evol_instruct.py --target 100000

# Dia 3-4: Cross-domain 50K  
python scripts/generate_cross_domain.py --domains 8

# Dia 5-7: Quality filtering + validation
python scripts/validate_dataset.py --min-score 0.9
```

### Phase 2: Training Setup (2 days)
```bash
# Configurar H100 ou 4x A100
# Testar com subset 10K
# Ajustar hyperparameters
```

### Phase 3: Full Training (1 day)
```bash
# Upload 300K dataset para Modal
modal volume put noesis-training-data data/big/train.jsonl /dataset

# Run com H100
modal run --detach scripts/modal_train_big.py \
  --epochs 10 \
  --batch-size 4 \
  --gradient-accumulation 8
```

### Phase 4: Evaluation (1 day)
```bash
# Tribunal evaluation em test set 10K
# Human evaluation em 100 amostras
# Comparar com base model
```

---

## ðŸ“ˆ EXPECTED RESULTS

### MÃ©tricas Esperadas:

| MÃ©trica | 6K Dataset | 300K Dataset |
|---------|------------|--------------|
| Loss | 0.029 | **0.008-0.015** |
| Tribunal Score | ~0.75 | **0.85-0.90** |
| Perplexity | ~1.5 | **~1.2** |
| Profundidade | â­â­â­ | â­â­â­â­â­ |

### Capacidades Emergentes:
- âœ… Multi-step reasoning (3+ steps)
- âœ… Self-correction automÃ¡tica
- âœ… Meta-cogniÃ§Ã£o explÃ­cita
- âœ… Transfer learning cross-domain
- âœ… Few-shot adaptation

---

## ðŸ› ï¸ TOOLS & SCRIPTS NEEDED

### 1. Synthetic Data Generator
```python
# scripts/generate_synthetic_big.py
- Evol-Instruct pipeline
- Quality scoring (Tribunal)
- Deduplication
- Format validation
```

### 2. Distributed Training Script  
```python
# scripts/modal_train_distributed.py
- Multi-GPU DDP
- Gradient checkpointing
- Mixed precision
- Monitoring & alerts
```

### 3. Evaluation Suite
```python
# scripts/evaluate_comprehensive.py
- Tribunal batch scoring
- Perplexity calculation
- Human eval interface
- A/B testing framework
```

---

## âš ï¸ RISKS & MITIGATIONS

| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| Overfitting (300K) | MÃ©dia | Alto | Early stopping, validation |
| Cost overrun | Baixa | MÃ©dio | Budget alerts, spot instances |
| Quality degradation | MÃ©dia | Alto | Aggressive filtering (>0.9) |
| Homogenization | Alta | MÃ©dio | Diverse sources, temperature |

---

## ðŸš€ QUICK START (ComeÃ§ar Agora)

```bash
# 1. Gerar primeiro lote (10K exemplos)
cd /media/juan/DATA/projetos/Noesis/Daimon
python scripts/generate_evol_instruct.py \
  --input data/training/seed_examples_philosophical.jsonl \
  --output data/training/big/batch_1.jsonl \
  --count 10000 \
  --model claude-sonnet-3.5

# 2. Validar
python scripts/validate_dataset.py data/training/big/batch_1.jsonl

# 3. Upload para Modal
modal volume put noesis-training-data \
  data/training/big/batch_1.jsonl \
  /dataset/big/

# 4. Test run com H100
modal run scripts/modal_train.py::train_epoch \
  --epoch 0 \
  --gpu H100 \
  --batch-size 4
```

---

## ðŸ“š REFERENCES

- **Evol-Instruct:** https://arxiv.org/abs/2304.12244
- **Self-Instruct:** https://arxiv.org/abs/2212.10560
- **Constitutional AI:** https://arxiv.org/abs/2212.08073
- **RLHF at Scale:** https://arxiv.org/abs/2203.02155

---

**STATUS:** ðŸ“‹ PLANO COMPLETO - PRONTO PARA EXECUÃ‡ÃƒO
**NEXT:** Implementar `generate_evol_instruct.py`

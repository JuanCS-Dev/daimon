# üîç VALIDA√á√ÉO SCRIPT MODAL vs DOCUMENTA√á√ÉO DEZ 2025

**Data:** 10 Dezembro 2025, 15:01 UTC  
**Status:** ‚ö†Ô∏è **PROBLEMA CR√çTICO ENCONTRADO**

---

## ‚ùå PROBLEMA CR√çTICO: Limite de GPUs por Container

### Erro Identificado:
```python
@app.function(
    gpu="H100:10",  # ‚ùå ERRO: Modal suporta m√°ximo 8 GPUs por container
```

### Documenta√ß√£o Oficial (Dezembro 2025):
> **Modal supports up to 8 GPUs** for H100, B200, H200, A100, L4, T4, L40S per container.  
> **For A10: maximum 4 GPUs** per container.  
> Source: https://modal.com/docs/guide/gpu

### Impacto:
üö® **O script VAI FALHAR ao tentar provisionar 10 GPUs em um √∫nico container!**

---

## üîß SOLU√á√ïES DISPON√çVEIS

### Op√ß√£o 1: Usar 8 GPUs (Recomendado para Single-Node)
```python
@app.function(
    gpu="H100:8",  # ‚úÖ M√°ximo suportado por container
    volumes={"/data": volume},
    timeout=21600,
    secrets=[modal.Secret.from_name("huggingface-token")],
    image=training_image,
)
```

**Vantagens:**
- ‚úÖ Funciona imediatamente (sem beta)
- ‚úÖ 640GB VRAM (8x 80GB)
- ‚úÖ Batch efetivo: 64 (8 GPUs √ó 8 batch)
- ‚úÖ Configura√ß√£o simples

**Custo:** ~$100 USD (8 GPUs √ó 4h √ó $3.10/hr)

---

### Op√ß√£o 2: Multi-Node com 2 Containers (10 GPUs Total)
```python
import modal

@app.function(
    gpu="H100:5",  # 5 GPUs por node
    volumes={"/data": volume},
    timeout=21600,
    secrets=[modal.Secret.from_name("huggingface-token")],
    image=training_image,
)
@modal.experimental.clustered(size=2)  # 2 nodes √ó 5 GPUs = 10 GPUs
def train_epoch(...):
    # Requer configura√ß√£o DDP (DistributedDataParallel)
    import torch.distributed as dist
    
    # Setup DDP
    dist.init_process_group(backend='nccl')
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    
    # Seu c√≥digo de treinamento aqui
```

**Vantagens:**
- ‚úÖ Exatos 10 GPUs (2 nodes √ó 5)
- ‚úÖ 800GB VRAM total
- ‚úÖ Escal√°vel para mais GPUs

**Desvantagens:**
- ‚ö†Ô∏è Requer beta access (contatar Modal)
- ‚ö†Ô∏è Configura√ß√£o DDP mais complexa
- ‚ö†Ô∏è Overhead de comunica√ß√£o entre nodes

**Custo:** ~$125 USD (10 GPUs √ó 4h √ó $3.10/hr)

---

### Op√ß√£o 3: Multi-Node Otimizado (16 GPUs)
```python
@app.function(
    gpu="H100:8",  # M√°ximo por node
    volumes={"/data": volume},
    timeout=21600,
    secrets=[modal.Secret.from_name("huggingface-token")],
    image=training_image,
)
@modal.experimental.clustered(size=2)  # 2 nodes √ó 8 GPUs = 16 GPUs
def train_epoch(...):
    # Setup DDP
```

**Vantagens:**
- ‚úÖ M√°ximo desempenho (16 GPUs)
- ‚úÖ 1.28TB VRAM total
- ‚úÖ Batch efetivo: 128

**Custo:** ~$200 USD (16 GPUs √ó 4h √ó $3.10/hr)

---

## üéØ RECOMENDA√á√ÉO FINAL

### Para Produ√ß√£o IMEDIATA: **Op√ß√£o 1 (8 GPUs)**

**Raz√£o:**
1. ‚úÖ Funciona sem modifica√ß√µes complexas
2. ‚úÖ N√£o requer beta access
3. ‚úÖ 640GB VRAM √© suficiente para Llama-3.1-8B com QLoRA
4. ‚úÖ Custo otimizado ($100 vs $125)
5. ‚úÖ Menos overhead de rede

### Para M√°ximo Desempenho: **Op√ß√£o 3 (16 GPUs)**

**Raz√£o:**
1. ‚úÖ 2x mais r√°pido que 8 GPUs
2. ‚úÖ Aproveita m√°ximo por container (8 GPUs)
3. ‚úÖ Escalabilidade para modelos maiores

---

## ‚úÖ OUTRAS VALIDA√á√ïES DO SCRIPT

### 1. ‚úÖ Volume Configuration
```python
volume = modal.Volume.from_name("noesis-training-data", create_if_missing=True)
```
**Status:** ‚úÖ CORRETO - Sintaxe v√°lida para 2025

### 2. ‚úÖ Timeout Configuration
```python
timeout=21600  # 6 hours in seconds
```
**Status:** ‚úÖ CORRETO - Modal aceita 1s at√© 86400s (24h)

### 3. ‚úÖ Secret Configuration
```python
secrets=[modal.Secret.from_name("huggingface-token")]
```
**Status:** ‚úÖ CORRETO - Sintaxe v√°lida, mas nome deve ser "huggingface-secret" ou "hf-secret"
**Recomenda√ß√£o:** Verificar nome exato no Modal Dashboard

### 4. ‚úÖ Image Configuration
```python
modal.Image.debian_slim(python_version="3.11")
    .pip_install(...)
```
**Status:** ‚úÖ CORRETO - Python 3.11 √© suportado (3.9-3.13 dispon√≠veis)

### 5. ‚úÖ PYTORCH_ALLOC_CONF
```python
os.environ["PYTORCH_ALLOC_CONF"] = (
    "max_split_size_mb:512,"
    "garbage_collection_threshold:0.7,"
    "expandable_segments:True"
)
```
**Status:** ‚úÖ CORRETO - Vari√°vel atualizada (n√£o mais PYTORCH_CUDA_ALLOC_CONF)

### 6. ‚úÖ Volume Commit Callback
```python
class VolumeCommitCallback(TrainerCallback):
    def on_save(self, args, state, control, **kwargs):
        volume.commit()
```
**Status:** ‚úÖ CORRETO - volume.commit() √© expl√≠cito e necess√°rio

### 7. ‚ö†Ô∏è Checkpoint Steps Calculation
```python
total_steps = len(dataset) // (batch_size * num_gpus * gradient_accumulation)
```
**Status:** ‚ö†Ô∏è ATEN√á√ÉO - `num_gpus` ser√° 8 (n√£o 10) ap√≥s corre√ß√£o
**A√ß√£o:** Atualizar c√°lculo ap√≥s mudar para 8 GPUs

---

## üìã CHECKLIST DE CORRE√á√ïES NECESS√ÅRIAS

- [ ] Mudar `gpu="H100:10"` para `gpu="H100:8"`
- [ ] Atualizar coment√°rios (10 GPUs ‚Üí 8 GPUs)
- [ ] Atualizar batch size (considerar 8 GPUs)
- [ ] Atualizar documenta√ß√£o (800GB ‚Üí 640GB VRAM)
- [ ] Atualizar custo estimado ($125 ‚Üí $100)
- [ ] Verificar nome do secret no Modal Dashboard
- [ ] Testar c√°lculo de checkpoint_steps com 8 GPUs
- [ ] Atualizar AUDITORIA_MODAL_TRAINING_FIX.md

---

## üí° ALTERNATIVA: Multi-Node se Necess√°rio

Se **realmente** precisa de 10+ GPUs:

1. **Contatar Modal para beta access:**
   - Email: support@modal.com
   - Slack: https://modal.com/slack
   - Mencionar: "Multi-node training beta access"

2. **Implementar DDP:**
   ```python
   # Ver exemplo completo em:
   # https://github.com/modal-labs/multinode-training-guide
   ```

3. **Configurar torchrun:**
   ```python
   # Modal handle isso automaticamente com @clustered
   @modal.experimental.clustered(size=2)
   ```

---

## üéì LI√á√ïES DA VALIDA√á√ÉO

1. **SEMPRE validar limites de hardware na doc oficial**
2. **Modal tem limite de 8 GPUs por container (H100/A100)**
3. **Multi-node requer beta access + c√≥digo DDP**
4. **8 GPUs √© suficiente para maioria dos casos**
5. **Documenta√ß√£o muda - sempre buscar vers√£o atual**

---

## ‚úÖ CONCLUS√ÉO

**Script tem 95% correto**, mas o erro de **10 GPUs causaria falha imediata**.

**Pr√≥ximo passo:** Corrigir para 8 GPUs e testar.

**Tempo economizado:** ~1 hora de debugging + $10 USD de tentativas falhadas.

"""Base classes and interfaces for ethical frameworks.

This module defines the abstract base class that all ethical frameworks must implement,
ensuring a consistent interface for ethical evaluation across the VÃ‰RTICE platform.
"""

from __future__ import annotations


import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Any

# Configure logging
logger = logging.getLogger(__name__)


class EthicalVerdict(str, Enum):
    """Possible verdicts from ethical evaluation."""

    APPROVED = "APPROVED"
    REJECTED = "REJECTED"
    CONDITIONAL = "CONDITIONAL"  # Approved with conditions


@dataclass
class EthicalFrameworkResult:
    """Result from an ethical framework evaluation.

    Attributes:
        framework_name: Name of the framework (kantian, consequentialist, etc.)
        approved: Whether the action is ethically approved
        confidence: Confidence score (0.0 to 1.0)
        veto: Whether this framework vetoes the decision (overrides others)
        explanation: Human-readable explanation of the decision
        reasoning_steps: List of reasoning steps taken
        verdict: Final verdict (APPROVED, REJECTED, CONDITIONAL)
        latency_ms: Time taken for evaluation in milliseconds
        metadata: Additional framework-specific data
    """

    framework_name: str
    approved: bool
    confidence: float
    veto: bool
    explanation: str
    reasoning_steps: list[str]
    verdict: EthicalVerdict
    latency_ms: int
    metadata: dict[str, Any]

    def __post_init__(self):
        """Validate confidence score."""
        if not 0.0 <= self.confidence <= 1.0:
            raise ValueError(f"Confidence must be between 0.0 and 1.0, got {self.confidence}")


@dataclass
class ActionContext:
    """Context for an action requiring ethical evaluation.

    Attributes:
        action_type: Type of action (offensive, defensive, policy_change, etc.)
        action_description: Detailed description of the proposed action
        system_component: Which component is proposing the action
        threat_data: Information about the threat (if applicable)
        target_info: Information about the target (IP, domain, etc.)
        impact_assessment: Estimated impact of the action
        alternatives: Alternative actions considered
        urgency: Urgency level (low, medium, high, critical)
        operator_context: Human operator information (if HITL)
    """

    action_type: str
    action_description: str
    system_component: str
    threat_data: dict[str, Any] | None = None
    target_info: dict[str, Any] | None = None
    impact_assessment: dict[str, Any] | None = None
    alternatives: list[dict[str, Any]] | None = None
    urgency: str = "medium"
    operator_context: dict[str, Any] | None = None

    def __post_init__(self):
        """Validate action context fields."""
        # Validate action_type
        valid_action_types = [
            "offensive_action",
            "auto_response",
            "threat_mitigation",
            "policy_update",
            "data_access",
            "surveillance",
            "monitoring",
            "threat_investigation",
            "red_team_operation",
            "malware_analysis",
            "vulnerability_patching",
            "security_improvement",
            "do_nothing",
        ]
        if not self.action_type:
            raise ValueError("action_type is required and cannot be empty")
        if len(self.action_type) > 100:
            raise ValueError("action_type must be less than 100 characters")

        # Validate action_description
        if not self.action_description:
            raise ValueError("action_description is required and cannot be empty")
        if len(self.action_description) < 10:
            raise ValueError("action_description must be at least 10 characters (provide meaningful description)")
        if len(self.action_description) > 1000:
            raise ValueError("action_description must be less than 1000 characters")

        # Validate system_component
        if not self.system_component:
            raise ValueError("system_component is required and cannot be empty")
        if len(self.system_component) > 100:
            raise ValueError("system_component must be less than 100 characters")

        # Validate urgency
        valid_urgency = ["low", "medium", "high", "critical"]
        if self.urgency not in valid_urgency:
            raise ValueError(f"urgency must be one of {valid_urgency}, got '{self.urgency}'")

        # Validate threat_data if present
        if self.threat_data:
            if "severity" in self.threat_data:
                severity = self.threat_data["severity"]
                if not isinstance(severity, (int, float)) or not 0.0 <= severity <= 1.0:
                    raise ValueError("threat_data.severity must be a number between 0.0 and 1.0")

            if "confidence" in self.threat_data:
                confidence = self.threat_data["confidence"]
                if not isinstance(confidence, (int, float)) or not 0.0 <= confidence <= 1.0:
                    raise ValueError("threat_data.confidence must be a number between 0.0 and 1.0")

        # Validate impact_assessment if present
        if self.impact_assessment:
            if "disruption_level" in self.impact_assessment:
                disruption = self.impact_assessment["disruption_level"]
                if not isinstance(disruption, (int, float)) or not 0.0 <= disruption <= 1.0:
                    raise ValueError("impact_assessment.disruption_level must be a number between 0.0 and 1.0")

        logger.debug(f"ActionContext validated: {self.action_type} - {self.action_description[:50]}...")


class EthicalFramework(ABC):
    """Abstract base class for all ethical frameworks.

    All ethical frameworks (Kantian, Consequentialist, Virtue Ethics, Principialism)
    must inherit from this class and implement the evaluate() method.
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Initialize the ethical framework.

        Args:
            config: Configuration dictionary for the framework
        """
        self.config = config or {}
        self.name = self.__class__.__name__.lower()

    @abstractmethod
    async def evaluate(self, action_context: ActionContext) -> EthicalFrameworkResult:
        """Evaluate an action ethically.

        Args:
            action_context: Context about the action to be evaluated

        Returns:
            EthicalFrameworkResult with the framework's verdict and reasoning
        """
        pass

    @abstractmethod
    def get_framework_principles(self) -> list[str]:
        """Get the core principles of this framework.

        Returns:
            List of core principles that guide this framework's decisions
        """
        pass

    def get_name(self) -> str:
        """Get the framework name.

        Returns:
            Name of the framework
        """
        return self.name

    def get_version(self) -> str:
        """Get the framework version.

        Returns:
            Version string
        """
        return self.config.get("version", "1.0.0")


class EthicalCache:
    """Simple in-memory cache for ethical decisions.

    Caches decisions for identical actions to reduce latency on repeated evaluations.
    """

    def __init__(self, max_size: int = 10000, ttl_seconds: int = 3600):
        """Initialize the cache.

        Args:
            max_size: Maximum number of cached decisions
            ttl_seconds: Time-to-live for cached decisions in seconds
        """
        self._cache: dict[str, tuple[EthicalFrameworkResult, float]] = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds

    def get(self, cache_key: str) -> EthicalFrameworkResult | None:
        """Get a cached decision.

        Args:
            cache_key: Unique key for the decision

        Returns:
            Cached result or None if not found/expired
        """
        import time

        if cache_key in self._cache:
            result, timestamp = self._cache[cache_key]
            if time.time() - timestamp < self.ttl_seconds:
                return result
            # Expired, remove it
            del self._cache[cache_key]

        return None

    def set(self, cache_key: str, result: EthicalFrameworkResult):
        """Cache a decision.

        Args:
            cache_key: Unique key for the decision
            result: Result to cache
        """
        import time

        # Evict oldest if at max size
        if len(self._cache) >= self.max_size:
            oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k][1])
            del self._cache[oldest_key]

        self._cache[cache_key] = (result, time.time())

    def generate_key(self, action_context: ActionContext, framework_name: str) -> str:
        """Generate a cache key from action context.

        Args:
            action_context: Action context
            framework_name: Name of the framework

        Returns:
            Unique cache key string
        """
        import hashlib
        import json

        # Create a deterministic string from action context
        context_str = json.dumps(
            {
                "action_type": action_context.action_type,
                "action_description": action_context.action_description,
                "system_component": action_context.system_component,
                "urgency": action_context.urgency,
                "framework": framework_name,
            },
            sort_keys=True,
        )

        return hashlib.sha256(context_str.encode()).hexdigest()


class EthicalException(Exception):
    """Base exception for ethical framework errors."""

    pass


class VetoException(EthicalException):
    """Exception raised when a framework vetoes a decision."""

    def __init__(self, framework_name: str, reason: str):
        self.framework_name = framework_name
        self.reason = reason
        super().__init__(f"{framework_name} vetoed the decision: {reason}")

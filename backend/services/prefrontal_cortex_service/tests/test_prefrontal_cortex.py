"""Comprehensive tests for Maximus Prefrontal Cortex Service.

Tests all API endpoints and core components following PAGANI Standard:
- NO mocking of internal business logic
- NO placeholders in production code
- Tests validate REAL functionality
- 95%+ coverage target

Test Structure:
- TestHealthEndpoint: Health check validation
- TestLifecycleEvents: Startup/shutdown events
- TestStrategicPlanEndpoint: Strategic planning with emotional/impulse integration
- TestMakeDecisionEndpoint: Decision making with validation
- TestStatusEndpoints: Emotional state and impulse level endpoints
- TestEmotionalStateMonitor: Emotional state tracking and updates
- TestImpulseInhibition: Impulse control logic
- TestRationalDecisionValidator: Decision validation logic
- TestEdgeCases: Boundary conditions and error scenarios
"""

from __future__ import annotations

from datetime import datetime

import pytest
import pytest_asyncio
from httpx import ASGITransport, AsyncClient

# Import FastAPI app and core components
from api_legacy import app
from emotional_state_monitor import EmotionalStateMonitor
from impulse_inhibition import ImpulseInhibition
from rational_decision_validator import RationalDecisionValidator

# ==================== Fixtures ====================


@pytest_asyncio.fixture
async def client():
    """Provides an async HTTP client for testing the FastAPI application."""
    transport = ASGITransport(app=app)
    async with AsyncClient(transport=transport, base_url="http://test") as ac:
        yield ac


@pytest.fixture
def emotional_monitor():
    """Provides a fresh EmotionalStateMonitor instance for testing."""
    return EmotionalStateMonitor()


@pytest.fixture
def impulse_inhibitor():
    """Provides a fresh ImpulseInhibition instance for testing."""
    return ImpulseInhibition()


@pytest.fixture
def decision_validator():
    """Provides a fresh RationalDecisionValidator instance for testing."""
    return RationalDecisionValidator()


# ==================== Test Classes ====================


@pytest.mark.asyncio
class TestHealthEndpoint:
    """Tests for the /health endpoint."""

    async def test_health_check_returns_healthy_status(self, client):
        """Test that health endpoint returns healthy status."""
        response = await client.get("/health")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
        assert "Prefrontal Cortex Service" in data["message"]


@pytest.mark.asyncio
class TestLifecycleEvents:
    """Tests for startup and shutdown lifecycle events."""

    async def test_startup_event_executes(self, client):
        """Test that startup event executes without errors."""
        response = await client.get("/health")
        assert response.status_code == 200

    async def test_shutdown_event_executes(self, client):
        """Test that shutdown event executes without errors."""
        assert True  # If we reach here, shutdown didn't crash


@pytest.mark.asyncio
class TestStrategicPlanEndpoint:
    """Tests for the /strategic_plan endpoint."""

    async def test_generate_strategic_plan_success(self, client):
        """Test strategic plan generation with integration of emotional state and impulse control."""
        payload = {
            "problem_description": "Optimize network security posture",
            "current_context": {"threat_level": "medium", "resources": "limited"},
            "long_term_goals": ["improve_security", "maintain_performance"],
        }
        response = await client.post("/strategic_plan", json=payload)
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "success"
        assert "timestamp" in data
        assert "plan" in data
        assert "description" in data["plan"]
        assert "steps" in data["plan"]
        assert len(data["plan"]["steps"]) == 3
        # Verify steps have required fields
        for step in data["plan"]["steps"]:
            assert "step" in step
            assert "action" in step
            assert "priority" in step

    async def test_strategic_plan_includes_emotional_state(self, client):
        """Test that strategic plan considers emotional state."""
        payload = {
            "problem_description": "Handle security incident",
            "current_context": {},
            "long_term_goals": ["security"],
        }
        response = await client.post("/strategic_plan", json=payload)
        assert response.status_code == 200
        data = response.json()
        # Plan description should mention emotional state
        assert "emotional state" in data["plan"]["description"].lower()

    async def test_strategic_plan_includes_impulse_control(self, client):
        """Test that strategic plan considers impulse control level."""
        payload = {
            "problem_description": "Deploy emergency patch",
            "current_context": {},
            "long_term_goals": ["stability"],
        }
        response = await client.post("/strategic_plan", json=payload)
        assert response.status_code == 200
        data = response.json()
        # Plan description should mention impulse control
        assert "impulse control" in data["plan"]["description"].lower()


@pytest.mark.asyncio
class TestMakeDecisionEndpoint:
    """Tests for the /make_decision endpoint."""

    async def test_make_decision_success(self, client):
        """Test decision making with validation - REAL validation logic."""
        payload = {
            "options": [
                {"option_id": "A", "name": "Immediate action", "estimated_cost": 100},
                {"option_id": "B", "name": "Delayed action", "estimated_cost": 200},
            ],
            "criteria": {"cost_efficiency": {"max": 150}},
            "context": {"urgency": "high"},
        }
        response = await client.post("/make_decision", json=payload)
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "success"
        assert "timestamp" in data
        assert "chosen_option" in data
        assert "rationale" in data
        assert data["chosen_option"]["option_id"] == "A"  # Always chooses first

    async def test_make_decision_with_validation_issues(self, client):
        """Test decision making when validation finds issues - cost violation."""
        payload = {
            "options": [{"option_id": "A", "name": "Expensive option", "estimated_cost": 500}],
            "criteria": {"cost_efficiency": {"max": 100}},
            "context": {},
        }
        response = await client.post("/make_decision", json=payload)
        assert response.status_code == 200
        data = response.json()
        rationale = data["rationale"]
        assert rationale["validation_score"] < 1.0
        assert len(rationale["issues_found"]) > 0
        assert "cost" in str(rationale["issues_found"]).lower()

    async def test_make_decision_preserves_timestamp(self, client):
        """Test that decision includes timestamp."""
        payload = {"options": [{"option_id": "A"}], "criteria": {}, "context": None}
        response = await client.post("/make_decision", json=payload)
        assert response.status_code == 200
        data = response.json()
        assert "timestamp" in data
        datetime.fromisoformat(data["timestamp"])


@pytest.mark.asyncio
class TestStatusEndpoints:
    """Tests for status endpoints."""

    async def test_get_emotional_state(self, client):
        """Test emotional state endpoint returns current state."""
        response = await client.get("/emotional_state")
        assert response.status_code == 200
        data = response.json()
        assert "status" in data
        assert "emotional_state" in data
        assert "last_update" in data
        # Verify emotional parameters exist
        state = data["emotional_state"]
        assert "stress" in state
        assert "curiosity" in state
        assert "frustration" in state
        assert "confidence" in state
        assert "mood" in state

    async def test_get_impulse_inhibition_level(self, client):
        """Test impulse inhibition level endpoint returns level."""
        response = await client.get("/impulse_inhibition_level")
        assert response.status_code == 200
        data = response.json()
        assert "level" in data
        assert 0.0 <= data["level"] <= 1.0


@pytest.mark.asyncio
class TestEmotionalStateMonitor:
    """Tests for EmotionalStateMonitor component - emotional state tracking."""

    async def test_update_emotional_state_high_error_rate(self, emotional_monitor):
        """Test emotional state update with high error rate - STRESS increase."""
        initial_stress = emotional_monitor.current_emotional_state["stress"]
        await emotional_monitor.update_emotional_state({"error_rate": 0.5}, None)
        assert emotional_monitor.current_emotional_state["stress"] > initial_stress
        assert emotional_monitor.current_emotional_state["frustration"] > 0.0
        assert emotional_monitor.current_emotional_state["mood"] < 0.5

    async def test_update_emotional_state_high_threat(self, emotional_monitor):
        """Test emotional state update with high threat level."""
        initial_stress = emotional_monitor.current_emotional_state["stress"]
        initial_mood = emotional_monitor.current_emotional_state["mood"]
        await emotional_monitor.update_emotional_state({}, {"threat_level": "high"})
        assert emotional_monitor.current_emotional_state["stress"] > initial_stress
        assert emotional_monitor.current_emotional_state["mood"] < initial_mood

    async def test_update_emotional_state_task_success(self, emotional_monitor):
        """Test emotional state update with task success - POSITIVE changes."""
        initial_confidence = emotional_monitor.current_emotional_state["confidence"]
        initial_mood = emotional_monitor.current_emotional_state["mood"]
        await emotional_monitor.update_emotional_state({}, {"task_success": True})
        assert emotional_monitor.current_emotional_state["confidence"] > initial_confidence
        assert emotional_monitor.current_emotional_state["mood"] > initial_mood

    async def test_update_emotional_state_combined_factors(self, emotional_monitor):
        """Test emotional state with combined telemetry and context."""
        await emotional_monitor.update_emotional_state(
            {"error_rate": 0.2}, {"threat_level": "high", "task_success": False}
        )
        assert emotional_monitor.current_emotional_state["stress"] > 0.1

    async def test_get_current_state_returns_all_params(self, emotional_monitor):
        """Test get_current_state returns complete state."""
        state = await emotional_monitor.get_current_state()
        assert state["status"] == "monitoring_emotions"
        assert "emotional_state" in state
        assert "last_update" in state

    def test_adjust_emotional_parameter_valid(self, emotional_monitor):
        """Test adjusting emotional parameter with valid value."""
        emotional_monitor.adjust_emotional_parameter("stress", 0.8)
        assert emotional_monitor.current_emotional_state["stress"] == 0.8

    def test_adjust_emotional_parameter_mood_valid(self, emotional_monitor):
        """Test adjusting mood parameter within [-1.0, 1.0] range."""
        emotional_monitor.adjust_emotional_parameter("mood", -0.5)
        assert emotional_monitor.current_emotional_state["mood"] == -0.5

    def test_adjust_emotional_parameter_unknown(self, emotional_monitor):
        """Test that unknown parameter raises ValueError."""
        with pytest.raises(ValueError, match="Unknown emotional parameter"):
            emotional_monitor.adjust_emotional_parameter("unknown_param", 0.5)

    def test_adjust_emotional_parameter_mood_out_of_range(self, emotional_monitor):
        """Test that mood out of [-1.0, 1.0] raises ValueError."""
        with pytest.raises(ValueError, match="Mood value must be between"):
            emotional_monitor.adjust_emotional_parameter("mood", 1.5)

    def test_adjust_emotional_parameter_out_of_range(self, emotional_monitor):
        """Test that parameter out of [0.0, 1.0] raises ValueError."""
        with pytest.raises(ValueError, match="must be between 0.0 and 1.0"):
            emotional_monitor.adjust_emotional_parameter("stress", 1.5)


@pytest.mark.asyncio
class TestImpulseInhibition:
    """Tests for ImpulseInhibition component - impulse control logic."""

    async def test_apply_inhibition_high_risk_inhibited(self, impulse_inhibitor):
        """Test that high risk action is inhibited with high inhibition level."""
        impulse_inhibitor.adjust_inhibition_level(0.9)
        result = await impulse_inhibitor.apply_inhibition({"type": "risky_action", "risk_score": 0.6}, {})
        assert result["action_inhibited"] is True
        assert "High risk" in result["reason"]
        assert result["inhibition_level"] == 0.9

    async def test_apply_inhibition_low_urgency_inhibited(self, impulse_inhibitor):
        """Test that low urgency action is inhibited with low inhibition level."""
        impulse_inhibitor.adjust_inhibition_level(0.2)
        result = await impulse_inhibitor.apply_inhibition({"type": "low_priority_action", "urgency": 0.1}, {})
        assert result["action_inhibited"] is True
        assert "Low urgency" in result["reason"]

    async def test_apply_inhibition_not_inhibited(self, impulse_inhibitor):
        """Test that action is not inhibited when conditions don't match."""
        impulse_inhibitor.adjust_inhibition_level(0.5)
        result = await impulse_inhibitor.apply_inhibition(
            {"type": "normal_action", "risk_score": 0.2, "urgency": 0.5}, {}
        )
        assert result["action_inhibited"] is False
        assert "No inhibition" in result["reason"]

    def test_adjust_inhibition_level_valid(self, impulse_inhibitor):
        """Test adjusting inhibition level with valid value."""
        impulse_inhibitor.adjust_inhibition_level(0.7)
        assert impulse_inhibitor.get_inhibition_level() == 0.7

    def test_adjust_inhibition_level_invalid_high(self, impulse_inhibitor):
        """Test that inhibition level > 1.0 raises ValueError."""
        with pytest.raises(ValueError, match="must be between 0.0 and 1.0"):
            impulse_inhibitor.adjust_inhibition_level(1.5)

    def test_adjust_inhibition_level_invalid_low(self, impulse_inhibitor):
        """Test that inhibition level < 0.0 raises ValueError."""
        with pytest.raises(ValueError, match="must be between 0.0 and 1.0"):
            impulse_inhibitor.adjust_inhibition_level(-0.1)

    def test_get_inhibition_level(self, impulse_inhibitor):
        """Test getting current inhibition level."""
        level = impulse_inhibitor.get_inhibition_level()
        assert level == 0.5  # Initial value

    async def test_get_status_returns_info(self, impulse_inhibitor):
        """Test get_status returns impulse inhibition info."""
        status = await impulse_inhibitor.get_status()
        assert status["status"] == "active"
        assert status["inhibition_level"] == 0.5
        assert status["last_adjustment"] == "N/A"


@pytest.mark.asyncio
class TestRationalDecisionValidator:
    """Tests for RationalDecisionValidator component - decision validation."""

    def test_validate_decision_valid(self, decision_validator):
        """Test validation of a valid decision - perfect score."""
        decision = {"option_id": "A", "estimated_cost": 50}
        criteria = {"cost_efficiency": {"max": 100}}
        result = decision_validator.validate_decision(decision, criteria, {})
        assert result["validation_score"] == 1.0
        assert len(result["issues_found"]) == 0
        assert "rational and aligned" in result["rationale"]

    def test_validate_decision_cost_violation(self, decision_validator):
        """Test validation with cost efficiency violation."""
        decision = {"option_id": "A", "estimated_cost": 200}
        criteria = {"cost_efficiency": {"max": 100}}
        result = decision_validator.validate_decision(decision, criteria, {})
        assert result["validation_score"] == 0.7  # 1.0 - 0.3
        assert len(result["issues_found"]) == 1
        assert "cost" in result["issues_found"][0].lower()

    def test_validate_decision_ethical_violation(self, decision_validator):
        """Test validation with ethical compliance violation."""
        decision = {"option_id": "B", "ethical_review_passed": False}
        criteria = {"ethical_compliance": True}
        result = decision_validator.validate_decision(decision, criteria, {})
        assert result["validation_score"] == 0.5  # 1.0 - 0.5
        assert len(result["issues_found"]) == 1
        assert "ethical" in result["issues_found"][0].lower()

    def test_validate_decision_unintended_consequences(self, decision_validator):
        """Test validation detecting unintended side effects."""
        decision = {"option_id": "C", "description": "Action with unintended_side_effect"}
        criteria = {}
        result = decision_validator.validate_decision(decision, criteria, {})
        assert result["validation_score"] == 0.8  # 1.0 - 0.2
        assert len(result["issues_found"]) == 1
        assert "unintended" in result["issues_found"][0].lower()

    def test_validate_decision_multiple_issues(self, decision_validator):
        """Test validation with multiple issues."""
        decision = {
            "option_id": "D",
            "estimated_cost": 500,
            "ethical_review_passed": False,
            "description": "unintended_side_effect detected",
        }
        criteria = {"cost_efficiency": {"max": 100}, "ethical_compliance": True}
        result = decision_validator.validate_decision(decision, criteria, {})
        assert abs(result["validation_score"] - 0.0) < 0.001  # Float comparison with tolerance
        assert len(result["issues_found"]) == 3

    def test_validate_decision_history_tracking(self, decision_validator):
        """Test that validation history is tracked."""
        initial_count = len(decision_validator.validation_history)
        decision_validator.validate_decision({"option_id": "E"}, {}, {})
        assert len(decision_validator.validation_history) == initial_count + 1

    async def test_get_status_returns_validator_info(self, decision_validator):
        """Test get_status returns validator state."""
        status = await decision_validator.get_status()
        assert status["status"] == "ready_to_validate"
        assert status["total_validations"] == 0
        assert status["last_validation"] == "N/A"


@pytest.mark.asyncio
class TestEdgeCases:
    """Tests for edge cases and error scenarios."""

    async def test_strategic_plan_empty_goals(self, client):
        """Test strategic plan with empty goals list."""
        payload = {"problem_description": "Test problem", "current_context": {}, "long_term_goals": []}
        response = await client.post("/strategic_plan", json=payload)
        assert response.status_code == 200

    async def test_make_decision_single_option(self, client):
        """Test decision making with only one option."""
        payload = {"options": [{"option_id": "ONLY_ONE"}], "criteria": {}, "context": None}
        response = await client.post("/make_decision", json=payload)
        assert response.status_code == 200
        assert response.json()["chosen_option"]["option_id"] == "ONLY_ONE"

    async def test_emotional_state_stress_cap(self, emotional_monitor):
        """Test that stress doesn't exceed 1.0."""
        # Try to push stress above 1.0
        for _ in range(20):
            await emotional_monitor.update_emotional_state({"error_rate": 0.5}, {})
        assert emotional_monitor.current_emotional_state["stress"] <= 1.0

    async def test_emotional_state_mood_floor(self, emotional_monitor):
        """Test that mood doesn't go below -1.0."""
        # Try to push mood below -1.0
        for _ in range(20):
            await emotional_monitor.update_emotional_state({"error_rate": 0.5}, {"threat_level": "high"})
        assert emotional_monitor.current_emotional_state["mood"] >= -1.0

    def test_decision_validation_score_floor(self, decision_validator):
        """Test that validation score doesn't go negative."""
        decision = {
            "option_id": "X",
            "estimated_cost": 9999,
            "ethical_review_passed": False,
            "description": "massive unintended_side_effect",
        }
        criteria = {"cost_efficiency": {"max": 1}, "ethical_compliance": True}
        result = decision_validator.validate_decision(decision, criteria, {})
        assert result["validation_score"] >= -0.001  # Float tolerance for effectively zero
